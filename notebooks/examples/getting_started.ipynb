{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with PyIceberg + Polars\n",
    "\n",
    "This notebook demonstrates how to query Iceberg tables managed by Nessie using PyIceberg and analyze the data with Polars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the required libraries and configure our connection to the Nessie catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "# Configuration from environment variables\n",
    "NESSIE_URI = os.environ.get(\"NESSIE_URI\", \"http://nessie.agartha-catalog.svc.cluster.local:19120/api/v2\")\n",
    "S3_ENDPOINT = os.environ.get(\"S3_ENDPOINT\", \"http://minio.agartha-storage.svc.cluster.local:9000\")\n",
    "S3_ACCESS_KEY = os.environ.get(\"S3_ACCESS_KEY_ID\", \"minioadmin\")\n",
    "S3_SECRET_KEY = os.environ.get(\"S3_SECRET_ACCESS_KEY\", \"minioadmin\")\n",
    "S3_WAREHOUSE = os.environ.get(\"S3_WAREHOUSE\", \"s3://agartha-warehouse\")\n",
    "\n",
    "print(f\"Nessie URI: {NESSIE_URI}\")\n",
    "print(f\"S3 Endpoint: {S3_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Nessie Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog(\n",
    "    \"nessie\",\n",
    "    **{\n",
    "        \"type\": \"rest\",\n",
    "        \"uri\": NESSIE_URI,\n",
    "        \"s3.endpoint\": S3_ENDPOINT,\n",
    "        \"s3.access-key-id\": S3_ACCESS_KEY,\n",
    "        \"s3.secret-access-key\": S3_SECRET_KEY,\n",
    "        \"warehouse\": S3_WAREHOUSE,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Connected to Nessie catalog!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available Namespaces and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List namespaces\n",
    "namespaces = catalog.list_namespaces()\n",
    "print(\"Available namespaces:\")\n",
    "for ns in namespaces:\n",
    "    print(f\"  - {ns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables in each namespace\n",
    "for ns in namespaces:\n",
    "    tables = catalog.list_tables(ns)\n",
    "    if tables:\n",
    "        print(f\"\\nTables in {ns}:\")\n",
    "        for table in tables:\n",
    "            print(f\"  - {table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an Iceberg Table into Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GitHub repositories table (adjust table name as needed)\n",
    "try:\n",
    "    table = catalog.load_table(\"raw.github_repositories\")\n",
    "    print(f\"Table: {table.name()}\")\n",
    "    print(f\"\\nSchema:\")\n",
    "    print(table.schema())\n",
    "except Exception as e:\n",
    "    print(f\"Could not load table: {e}\")\n",
    "    print(\"Make sure the table exists by running the Dagster pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan the table and convert to Polars DataFrame\n",
    "try:\n",
    "    arrow_table = table.scan().to_arrow()\n",
    "    df = pl.from_arrow(arrow_table)\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "    df.head(10)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Data with Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Group by language and compute statistics\n",
    "try:\n",
    "    language_stats = (\n",
    "        df.group_by(\"language\")\n",
    "        .agg([\n",
    "            pl.count().alias(\"repo_count\"),\n",
    "            pl.col(\"stargazers_count\").sum().alias(\"total_stars\"),\n",
    "            pl.col(\"forks_count\").sum().alias(\"total_forks\"),\n",
    "        ])\n",
    "        .sort(\"total_stars\", descending=True)\n",
    "    )\n",
    "    language_stats\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query with Column Selection and Filtering\n",
    "\n",
    "PyIceberg supports predicate pushdown for efficient queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns and filter\n",
    "try:\n",
    "    # Only fetch specific columns (projection pushdown)\n",
    "    scan = table.scan(\n",
    "        selected_fields=(\"name\", \"language\", \"stargazers_count\", \"forks_count\")\n",
    "    )\n",
    "    \n",
    "    df_subset = pl.from_arrow(scan.to_arrow())\n",
    "    \n",
    "    # Filter in Polars for repos with > 100 stars\n",
    "    popular_repos = df_subset.filter(pl.col(\"stargazers_count\") > 100)\n",
    "    print(f\"Repos with > 100 stars: {len(popular_repos)}\")\n",
    "    popular_repos.head(10)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Query via Trino\n",
    "\n",
    "You can also query tables through Trino for more complex SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trino.dbapi import connect\n",
    "\n",
    "TRINO_HOST = os.environ.get(\"TRINO_HOST\", \"trino.agartha-processing-trino.svc.cluster.local\")\n",
    "TRINO_PORT = int(os.environ.get(\"TRINO_PORT\", \"8080\"))\n",
    "\n",
    "try:\n",
    "    conn = connect(\n",
    "        host=TRINO_HOST,\n",
    "        port=TRINO_PORT,\n",
    "        user=\"jupyter\",\n",
    "        catalog=\"agartha\",\n",
    "        schema=\"raw\",\n",
    "    )\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SHOW TABLES\")\n",
    "    tables = cursor.fetchall()\n",
    "    print(\"Tables available via Trino:\")\n",
    "    for t in tables:\n",
    "        print(f\"  - {t[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not connect to Trino: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query via Trino and load into Polars\n",
    "try:\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT language, COUNT(*) as repo_count, SUM(stargazers_count) as total_stars\n",
    "        FROM raw.github_repositories\n",
    "        GROUP BY language\n",
    "        ORDER BY total_stars DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    \n",
    "    rows = cursor.fetchall()\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    \n",
    "    df_trino = pl.DataFrame(rows, schema=columns)\n",
    "    df_trino\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
