global:
  serviceAccountName: ${service_account_name}

dagsterWebserver:
  replicaCount: ${webserver_replica_num}
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  env:
    - name: NESSIE_URI
      value: "${nessie_uri}"
    - name: S3_ENDPOINT
      value: "${s3_endpoint}"
    - name: S3_WAREHOUSE
      value: "${s3_warehouse}"
    - name: S3_ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: ${s3_credentials_secret_name}
          key: S3_ACCESS_KEY_ID
    - name: S3_SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: ${s3_credentials_secret_name}
          key: S3_SECRET_ACCESS_KEY
    - name: S3_REGION
      value: "us-east-1"
    - name: S3_PATH_STYLE_ACCESS
      value: "true"
  # Custom workspace pointing to our user code server (managed outside subchart)
  workspace:
    enabled: true
    servers:
      - host: "dagster-agartha-pipelines"
        port: 3030
        name: "agartha-pipelines"

dagsterDaemon:
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1Gi
  env:
    - name: NESSIE_URI
      value: "${nessie_uri}"
    - name: S3_ENDPOINT
      value: "${s3_endpoint}"
    - name: S3_WAREHOUSE
      value: "${s3_warehouse}"
    - name: S3_ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: ${s3_credentials_secret_name}
          key: S3_ACCESS_KEY_ID
    - name: S3_SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: ${s3_credentials_secret_name}
          key: S3_SECRET_ACCESS_KEY
    - name: S3_REGION
      value: "us-east-1"
    - name: S3_PATH_STYLE_ACCESS
      value: "true"

runCoordinator:
  enabled: true
  type: QueuedRunCoordinator
  config:
    queuedRunCoordinator:
      maxConcurrentRuns: ${max_concurrent_runs}

runLauncher:
  type: K8sRunLauncher
  config:
    k8sRunLauncher:
      image:
        repository: "docker.io/dagster/dagster-k8s"
        tag: "1.12.11"
        pullPolicy: IfNotPresent
      envConfigMaps:
        - name: ${storage_config_map_name}
        - name: ${spark_config_map_name}
        - name: ${flink_config_map_name}
      envSecrets:
        - name: ${s3_credentials_secret_name}
      envVars:
        - "DLT_S3_BUCKET=agartha-raw"
        - "DAGSTER_CLI_API_GRPC_LAZY_LOAD_USER_CODE=1"
      volumes:
        - name: dagster-code
          configMap:
            name: ${user_code_config_map_name}
        - name: dlt-state
          emptyDir: {}
      volumeMounts:
        - name: dagster-code
          mountPath: /opt/dagster/app
        - name: dlt-state
          mountPath: /tmp/dlt_pipelines
      runK8sConfig:
        containerConfig:
          resources:
            requests:
              cpu: 250m
              memory: 512Mi
            limits:
              cpu: 1000m
              memory: 2Gi
          # Override command to install dlt before running dagster
          command:
            - "/bin/sh"
            - "-c"
            - |
              pip install --no-cache-dir "dlt[filesystem]>=1.0.0" "s3fs>=2024.2.0" "pyarrow>=15.0.0" "boto3" && exec "$@"
            - "--"

postgresql:
  enabled: true
  auth:
    existingSecret: ${postgres_existing_secret}
    secretKeys:
      userPasswordKey: password
      adminPasswordKey: postgres-password
  primary:
    persistence:
      enabled: true
      size: 8Gi

rabbitmq:
  enabled: false

redis:
  enabled: false

# Keep workspace enabled but disable subchart deployment
# (we manage user code deployment separately)
dagster-user-deployments:
  enabled: true
  enableSubchart: false

serviceAccount:
  create: false
  name: ${service_account_name}
